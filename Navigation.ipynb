{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is a solution to the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from p1_navigation.agents import DQN_Agent, DoubleDQN_Agent, Dueling_DDQN_Agent\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"C:\\Dev\\Python\\RL\\deep-reinforcement-learning\\p1_navigation\\Banana_Windows_x86_64\\Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Actions and learn in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simulation(Agent, num_episodes=1500, num_sims=5, eps_start=1.0, eps_end=0.005, eps_decay=0.995):\n",
    "    \"\"\"Run simulations in the environment and learn from the experiences. This will train the Agent.\n",
    "\n",
    "    :param Agent: (class) This is the class of Agent from agents.py that you want to use\n",
    "    :param num_episodes: (int) maximum number of training episodes\n",
    "    :param num_sims: (int) How many simulations to run for this Agent to get average scores\n",
    "    :param eps_start: (float) starting value of epsilon, for epsilon-greedy action selection\n",
    "    :param eps_end: (float) minimum value of epsilon\n",
    "    :param eps_decay: (float) multiplicative factor (per episode) for decreasing epsilon\n",
    "    :return: list of scores achieved over the simulation\n",
    "    \"\"\"\n",
    "    scores = [0]*num_episodes\n",
    "    for sim in range(1,num_sims+1):\n",
    "        # Reset the Agent and epsilon\n",
    "        agent = Agent(state_size, action_size, seed=88)\n",
    "        eps = eps_start\n",
    "        for episode_num in range(1,num_episodes+1):\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            state = env_info.vector_observations[0]\n",
    "            score = 0\n",
    "            while True:\n",
    "                action = int(agent.act(state,eps))\n",
    "                env_info = env.step(action)[brain_name]\n",
    "                next_state = env_info.vector_observations[0]\n",
    "                reward = env_info.rewards[0]\n",
    "                done = env_info.local_done[0]\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            # Episode finished\n",
    "            scores[episode_num-1] += score\n",
    "            eps = max(eps*eps_decay, eps_end)\n",
    "            print(f\"Sim: {sim}, Episode: {episode_num}, Score: {score}\")\n",
    "    #Average scores for the number of simulations\n",
    "    scores = [score/num_sims for score in scores]\n",
    "\n",
    "    # All episodes finished\n",
    "    torch.save(agent.qnetwork_local.state_dict(), f\"C:\\Dev\\Python\\RL\\\\Navigation_Project\\p1_navigation\\checkpoints\\\\{agent}.pth\")\n",
    "    #Save scores to file so we do not need to rerun the entire simulation\n",
    "    f = open(f\"C:\\Dev\\Python\\RL\\\\Navigation_Project\\p1_navigation\\saved_scores\\\\{agent}.txt\", 'w')\n",
    "    s1 = '\\n'.join([str(s) for s in scores])\n",
    "    f.write(s1)\n",
    "    f.close()\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Setup what sort of simulation you want\n",
    "train_dqn = False\n",
    "train_double_dqn = False\n",
    "train_dueling = False\n",
    "if train_dqn:\n",
    "    train_simulation(DQN_Agent)\n",
    "if train_double_dqn:\n",
    "    train_simulation(DoubleDQN_Agent)\n",
    "if train_dueling:\n",
    "    train_simulation(Dueling_DDQN_Agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Watch Trained Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def watch_trained(Agent, env):\n",
    "    # Load in trained Agent\n",
    "    agent = Agent(state_size, action_size, 88)\n",
    "    agent.qnetwork_local.load_state_dict(torch.load(f\"C:\\Dev\\Python\\RL\\\\Navigation_Project\\p1_navigation\\checkpoints\\\\{agent}.pth\"))\n",
    "\n",
    "    # Run the simulation and print the score\n",
    "    score = 0\n",
    "    eps = 0 #No exploration on trained agent\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    while True:\n",
    "        action = int(agent.act(state,eps))\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        score += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode Score: {score}\")\n",
    "\n",
    "watch_trained(DQN_Agent, env)\n",
    "env.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def graph_results(Agents, save_graph=True):\n",
    "    \"\"\" Used to graph the results of agents from simulations\n",
    "\n",
    "    :param agent_scores: list of Class Agents you want to graph results from\n",
    "    :param save_graph: bool used to save the graph in images\n",
    "    \"\"\"\n",
    "    # Load scores for given Agents\n",
    "    agent_scores = []\n",
    "    for agent in Agents:\n",
    "        if agent == DQN_Agent:\n",
    "            class_name = \"DQN_Agent\"\n",
    "        elif agent == DoubleDQN_Agent:\n",
    "            class_name = \"Double_DQN_Agent\"\n",
    "        elif agent == Dueling_DDQN_Agent:\n",
    "            class_name = \"Dueling_DDQN_Agent\"\n",
    "        with open(f\"C:\\Dev\\Python\\RL\\\\Navigation_Project\\p1_navigation\\saved_scores\\\\{class_name}.txt\", 'r') as f:\n",
    "            scores = f.read().splitlines()\n",
    "            scores = [float(score) for score in scores]\n",
    "            agent_scores.append((class_name, scores))\n",
    "\n",
    "    # Setup up graph\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(xlabel=\"Episode #\", ylabel='Score', title=\"Navigation Agent Learning Rate\")\n",
    "    ax.grid()\n",
    "    save_name = \"\"\n",
    "    # Graph all the results\n",
    "    for name, score in agent_scores:\n",
    "        #Plot Scores\n",
    "        ax.plot(np.arange(len(score)), score, label=f\"{name}\")\n",
    "        save_name += f\"{name}_\"\n",
    "        # Plot rolling average\n",
    "        rolling_average = np.convolve(score, np.ones(100)/100)\n",
    "        rolling_average = rolling_average[:-100] #Removes tail that is window_size < 100\n",
    "        ax.plot(np.arange(len(rolling_average)), rolling_average, label=f\"Average per 100 episodes\")\n",
    "    # Plot solved line\n",
    "    ax.plot(np.arange(len(agent_scores[0][1])), np.ones(len(agent_scores[0][1]))*13, color='black', linestyle='dashed', label=\"Solved\")\n",
    "    ax.legend()\n",
    "\n",
    "    if save_graph:\n",
    "        filepath = f\"C:\\Dev\\Python\\RL\\\\Navigation_Project\\p1_navigation\\images\\\\{save_name}.png\"\n",
    "        fig.savefig(filepath)\n",
    "    plt.show()\n",
    "\n",
    "# Setup what sort of graphs you want\n",
    "graph_dqn = False\n",
    "graph_double_dqn = False\n",
    "graph_dueling = False\n",
    "graph_comparison = False\n",
    "if graph_dqn:\n",
    "    graph_results([DQN_Agent])\n",
    "if graph_double_dqn:\n",
    "    graph_results([DoubleDQN_Agent])\n",
    "if graph_dueling:\n",
    "    graph_results([Dueling_DDQN_Agent])\n",
    "if graph_comparison:\n",
    "    graph_results([DQN_Agent, DoubleDQN_Agent, Dueling_DDQN_Agent])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}